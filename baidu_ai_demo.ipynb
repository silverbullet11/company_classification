{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 你的 APPID AK SK \"\"\"\n",
    "APP_ID = '11361253'\n",
    "API_KEY = 'umkSZ5a0q2Lql2sAFSFZYcxu'\n",
    "SECRET_KEY = 'v6dkpDbS75bD76alu0inaWtUOwvlKvKO'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"log_id\": 3244810135947090889, \"text\": \"机械力转换设备是技术成熟待市场开发并产业化的产品，将成为公司未来收入的重要支撑\", \"items\": [{\"loc_details\": [], \"byte_offset\": 0, \"uri\": \"\", \"pos\": \"n\", \"ne\": \"\", \"item\": \"机械\", \"basic_words\": [\"机械\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 4, \"uri\": \"\", \"pos\": \"n\", \"ne\": \"\", \"item\": \"力\", \"basic_words\": [\"力\"], \"byte_length\": 2, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 6, \"uri\": \"\", \"pos\": \"vn\", \"ne\": \"\", \"item\": \"转换\", \"basic_words\": [\"转换\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 10, \"uri\": \"\", \"pos\": \"n\", \"ne\": \"\", \"item\": \"设备\", \"basic_words\": [\"设备\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 14, \"uri\": \"\", \"pos\": \"v\", \"ne\": \"\", \"item\": \"是\", \"basic_words\": [\"是\"], \"byte_length\": 2, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 16, \"uri\": \"\", \"pos\": \"n\", \"ne\": \"\", \"item\": \"技术\", \"basic_words\": [\"技术\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 20, \"uri\": \"\", \"pos\": \"a\", \"ne\": \"\", \"item\": \"成熟\", \"basic_words\": [\"成熟\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 24, \"uri\": \"\", \"pos\": \"v\", \"ne\": \"\", \"item\": \"待\", \"basic_words\": [\"待\"], \"byte_length\": 2, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 26, \"uri\": \"\", \"pos\": \"n\", \"ne\": \"\", \"item\": \"市场\", \"basic_words\": [\"市场\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 30, \"uri\": \"\", \"pos\": \"vn\", \"ne\": \"\", \"item\": \"开发\", \"basic_words\": [\"开发\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 34, \"uri\": \"\", \"pos\": \"c\", \"ne\": \"\", \"item\": \"并\", \"basic_words\": [\"并\"], \"byte_length\": 2, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 36, \"uri\": \"\", \"pos\": \"v\", \"ne\": \"\", \"item\": \"产业化\", \"basic_words\": [\"产业\", \"化\"], \"byte_length\": 6, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 42, \"uri\": \"\", \"pos\": \"u\", \"ne\": \"\", \"item\": \"的\", \"basic_words\": [\"的\"], \"byte_length\": 2, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 44, \"uri\": \"\", \"pos\": \"n\", \"ne\": \"\", \"item\": \"产品\", \"basic_words\": [\"产品\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 48, \"uri\": \"\", \"pos\": \"w\", \"ne\": \"\", \"item\": \"，\", \"basic_words\": [\"，\"], \"byte_length\": 2, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 50, \"uri\": \"\", \"pos\": \"d\", \"ne\": \"\", \"item\": \"将\", \"basic_words\": [\"将\"], \"byte_length\": 2, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 52, \"uri\": \"\", \"pos\": \"v\", \"ne\": \"\", \"item\": \"成为\", \"basic_words\": [\"成为\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 56, \"uri\": \"\", \"pos\": \"n\", \"ne\": \"\", \"item\": \"公司\", \"basic_words\": [\"公司\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 60, \"uri\": \"\", \"pos\": \"t\", \"ne\": \"\", \"item\": \"未来\", \"basic_words\": [\"未来\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 64, \"uri\": \"\", \"pos\": \"n\", \"ne\": \"\", \"item\": \"收入\", \"basic_words\": [\"收入\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 68, \"uri\": \"\", \"pos\": \"u\", \"ne\": \"\", \"item\": \"的\", \"basic_words\": [\"的\"], \"byte_length\": 2, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 70, \"uri\": \"\", \"pos\": \"a\", \"ne\": \"\", \"item\": \"重要\", \"basic_words\": [\"重要\"], \"byte_length\": 4, \"formal\": \"\"}, {\"loc_details\": [], \"byte_offset\": 74, \"uri\": \"\", \"pos\": \"vn\", \"ne\": \"\", \"item\": \"支撑\", \"basic_words\": [\"支撑\"], \"byte_length\": 4, \"formal\": \"\"}]}'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '‐机械力转换设备是技术成熟待市场开发并产业化的产品，将成为公司未来收入的重要支撑'.encode('gbk').decode('gbk')\n",
    "# ®\n",
    "\"\"\" 调用词法分析 \"\"\"\n",
    "request_url = 'https://aip.baidubce.com/rpc/2.0/nlp/v1/lexer?access_token=' + ACCESS_TOKEN\n",
    "headers = {'Content-Type': 'application/json; charset=UTF-8'}\n",
    "\n",
    "body = {\n",
    "    \"text\": text\n",
    "}\n",
    "results = requests.post(url=request_url, headers=headers, data=json.dumps(body))\n",
    "results_json = json.loads(results.text)\n",
    "results.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sys, os\n",
    "import pandas as pd\n",
    "training_csv = '~/!Final_Project/training.csv'\n",
    "testing_csv = '~/!Final_Project/testing.csv'\n",
    "df_train = pd.read_csv(training_csv, names=['class', 'description'])\n",
    "df_test = pd.read_csv(testing_csv, names=['index', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Baidu results to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"log_id\": 8879780789704476338, \"error_code\": 282002, \"error_msg\": \"input encoding error\"}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_json = json.loads(results.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8f198f15e234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'items'"
     ]
    }
   ],
   "source": [
    "word_tokens = [i['item'] for i in results_json['items']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(word_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_train[0:10]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '\"', '#', '$', '%']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/alvin/!Final_Project/stop_words.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "stop_words = [x[:-1] for x in lines]\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "ACCESS_TOKEN = ''\n",
    "\n",
    "def refresh_access_token():\n",
    "    host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=' + API_KEY + '&client_secret=' + SECRET_KEY\n",
    "    headers = {'Content-Type': 'application/json; charset=UTF-8'}\n",
    "\n",
    "    response = requests.get(host, headers=headers)\n",
    "    resp_json = json.loads(response.text)\n",
    "    global ACCESS_TOKEN\n",
    "    ACCESS_TOKEN = resp_json['access_token']\n",
    "    print('ACCESS_TOKEN: ', ACCESS_TOKEN)\n",
    "    return resp_json['access_token']\n",
    "\n",
    "\n",
    "def get_tokens(sentence):\n",
    "    if '®' in sentence:\n",
    "        sentence = sentence.replace('®', '')\n",
    "    if '‚' in sentence:\n",
    "        sentence = sentence.replace(',', '')\n",
    "    if '‛' in sentence:\n",
    "        sentence = sentence.replace('‛', '')\n",
    "    if '‐' in sentence:\n",
    "        sentence = sentence.replace('‐', '')\n",
    "    request_url = 'https://aip.baidubce.com/rpc/2.0/nlp/v1/lexer?access_token=' + ACCESS_TOKEN\n",
    "    headers = {'Content-Type': 'application/json; charset=UTF-8'}\n",
    "\n",
    "    body = {\n",
    "        \"text\": sentence\n",
    "    }\n",
    "#     print(\"{0}\\tSentence: {1}\".format(datetime.datetime.now().strftime(\"%X\"), sentence))\n",
    "    results = requests.post(url=request_url, headers=headers, data=json.dumps(body))\n",
    "    results_json = json.loads(results.text)\n",
    "    try:\n",
    "        word_tokens = [i['item'] for i in results_json['items'] if i['item'] not in stop_words]\n",
    "    except:\n",
    "        print(\"Sentence: \", sentence)\n",
    "        print(results.text)\n",
    "#     print(\"{0}\\tTokens: {1}\".format(datetime.datetime.now().strftime(\"%X\"), word_tokens))\n",
    "    return word_tokens\n",
    "\n",
    "\n",
    "# get_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.now().strftime(\"%X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['tokens'] = df_test['description'].apply(lambda x: get_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['tokens'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tokens'] = df_train['description'].apply(lambda x: get_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS_TOKEN:  24.33c2a0d98baac4db19f9eb3fcf5d3534.2592000.1531001899.282335-11361253\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "ACCESS_TOKEN:  24.6d626dc4a9b261d6b610440623a0b338.2592000.1531001960.282335-11361253\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "ACCESS_TOKEN:  24.134f25da739a2386978995c80ba817c8.2592000.1531002022.282335-11361253\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "ACCESS_TOKEN:  24.edec7fddbae84061754149bf58198834.2592000.1531002082.282335-11361253\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "ACCESS_TOKEN:  24.f7545f515352544cc9031a4462675b49.2592000.1531002146.282335-11361253\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "Sentence:  公司专业从事除铁器和永磁‐机械力转换设备等磁力类设备以及部分电气类产品的研发、设计、生产与销售。除铁器是公司的成熟产品和核心产品，是目前公司主营业务收入的主要来源；起重永磁铁、永磁耦合联轴器、永磁耦合调速器等永磁‐机械力转换设备是技术成熟待市场开发并产业化的产品，将成为公司未来收入的重要支撑；公司电气类产品主要为管型母线、电缆桥架，其中管型母线是公司的技术储备产品。自成立以来，公司的主营业务和主要产品未发生重大变化。\n",
      "{\"log_id\": 1075494389965071628, \"error_code\": 282004, \"error_msg\": \"invalid parameter(s)\"}\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'word_tokens' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-36d16452f747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrefresh_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchunk_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3192\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-36d16452f747>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrefresh_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchunk_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-270c2c9c323a>\u001b[0m in \u001b[0;36mget_tokens\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#     print(\"{0}\\tTokens: {1}\".format(datetime.datetime.now().strftime(\"%X\"), word_tokens))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'word_tokens' referenced before assignment"
     ]
    }
   ],
   "source": [
    "chunk_size = 10\n",
    "i = 0\n",
    "tokens = []\n",
    "for chunk in pd.read_csv(testing_csv, chunksize=chunk_size, names=['class', 'description'], skiprows=1755):\n",
    "    if i % 100 == 0:\n",
    "        refresh_access_token()\n",
    "    chunk_tokens = chunk['description'].apply(lambda x: get_tokens(x))\n",
    "    tokens.append(chunk_tokens)\n",
    "    i += 10\n",
    "    print(str(i))\n",
    "#     if i==20:\n",
    "#         break\n",
    "\n",
    "print(tokens[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 5\n",
    "i = 0\n",
    "tokens_train = []\n",
    "for chunk in pd.read_csv(training_csv, chunksize=chunk_size, names=['class', 'description']):\n",
    "    if i % 100 == 0:\n",
    "        refresh_access_token()\n",
    "    chunk_tokens = chunk['description'].apply(lambda x: get_tokens(x))\n",
    "    tokens_train.append(chunk_tokens)\n",
    "    i += 5\n",
    "    print(str(i))\n",
    "#     if i==20:\n",
    "#         break\n",
    "\n",
    "print(tokens_train[-5:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
