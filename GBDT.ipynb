{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvin/.conda/envs/tf-gpu/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/alvin/.conda/envs/tf-gpu/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, CustomJS, HoverTool\n",
    "from bokeh.io import output_notebook, push_notebook\n",
    "from bokeh.layouts import gridplot, widgetbox, layout\n",
    "from bokeh.models.widgets import Select\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Spectral6, Spectral11\n",
    "from bokeh.models.widgets import Select\n",
    "\n",
    "from pipelines import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "training_path = '/home/alvin/!Final_Project/training_with_tokens.xlsx'\n",
    "testing_path = '/home/alvin/!Final_Project/testing_with_tokens.xlsx'\n",
    "\n",
    "embedding_dim = 10\n",
    "top_n_token = 10\n",
    "\n",
    "print('Load data...')\n",
    "df_train = load_data(training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[合晟资产, 专注, 股票, 债券, 二级市场, 投资, 合格, 投资者, 资产, 管理, ...</td>\n",
       "      <td>合晟资产 专注 股票 债券 二级市场 投资 合格 投资者 资产 管理 企业 业务范围 资产 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[中, 小微企业, 个体, 工商户, 农户, 贷款, 设立, 发生, 变化, UNKNOWN]</td>\n",
       "      <td>中 小微企业 个体 工商户 农户 贷款 设立 发生 变化 UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[立足于, 商业地产, 商业地产, 开发, 销售, 运营, 全产业链, 一整套, 增值, 业...</td>\n",
       "      <td>立足于 商业地产 商业地产 开发 销售 运营 全产业链 一整套 增值 业务 覆盖 商业 定位...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[工商管理部门, 核准, 经营范围, 投资, 咨询, 经济, 信息, 咨询, 企业管理, 咨...</td>\n",
       "      <td>工商管理部门 核准 经营范围 投资 咨询 经济 信息 咨询 企业管理 咨询 品牌 推广 策划...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[中国, 境内, 港, 澳, 台, 保险代理, 销售, 研究, 能力, 专业化, 能力, 团...</td>\n",
       "      <td>中国 境内 港 澳 台 保险代理 销售 研究 能力 专业化 能力 团体 个人保险 受众 投保...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                             tokens  \\\n",
       "0      2  [合晟资产, 专注, 股票, 债券, 二级市场, 投资, 合格, 投资者, 资产, 管理, ...   \n",
       "1      2    [中, 小微企业, 个体, 工商户, 农户, 贷款, 设立, 发生, 变化, UNKNOWN]   \n",
       "2      1  [立足于, 商业地产, 商业地产, 开发, 销售, 运营, 全产业链, 一整套, 增值, 业...   \n",
       "3      2  [工商管理部门, 核准, 经营范围, 投资, 咨询, 经济, 信息, 咨询, 企业管理, 咨...   \n",
       "4      2  [中国, 境内, 港, 澳, 台, 保险代理, 销售, 研究, 能力, 专业化, 能力, 团...   \n",
       "\n",
       "                                            sentence  \n",
       "0  合晟资产 专注 股票 债券 二级市场 投资 合格 投资者 资产 管理 企业 业务范围 资产 ...  \n",
       "1               中 小微企业 个体 工商户 农户 贷款 设立 发生 变化 UNKNOWN  \n",
       "2  立足于 商业地产 商业地产 开发 销售 运营 全产业链 一整套 增值 业务 覆盖 商业 定位...  \n",
       "3  工商管理部门 核准 经营范围 投资 咨询 经济 信息 咨询 企业管理 咨询 品牌 推广 策划...  \n",
       "4  中国 境内 港 澳 台 保险代理 销售 研究 能力 专业化 能力 团体 个人保险 受众 投保...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokensPicker(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, embed_dim=10, top_n=10, window=5, min_count=1):\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.top_n_token = top_n\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "\n",
    "    def generate_word_embedding_from_df(self, df, col='tokens', vect_dims=100, window=5, min_count=1, workers=4, seed=11):\n",
    "        saved_model_name = w2v_file_name_from_parameters(col, vect_dims, window, min_count, seed)\n",
    "        if os.path.exists(saved_model_name):\n",
    "            return Word2Vec.load(saved_model_name)\n",
    "        else:\n",
    "            all_token_lists = df[col]\n",
    "            # size: The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token(word)\n",
    "            # sg: THe training algorithm, either CBOW(0) or skip gram(1).\n",
    "            # window: The maximum distance between a target word and words around the target word.\n",
    "            # min_count: The minimum count of words to consider when training the model; words with an occurence less than this count will be ignored.\n",
    "            w2v_model = Word2Vec(sentences=all_token_lists, size=vect_dims, sg=1, window=window, min_count=min_count,\n",
    "                                 seed=seed, workers=workers)\n",
    "            w2v_model.save(saved_model_name)\n",
    "            return w2v_model\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        self.w2v_model = self.generate_word_embedding_from_df(df,\n",
    "                                                    col='tokens',\n",
    "                                                    vect_dims=self.embedding_dim\n",
    "                                                    )\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectors = self.vectorizer.fit_transform(df['sentence'].tolist())\n",
    "        return self\n",
    "\n",
    "    def get_top_tokens_in_doc(self, df, vectors, row_id, top_n=25):\n",
    "        row = np.squeeze(vectors[row_id].toarray())\n",
    "        tokens = df.loc[row_id]['tokens']\n",
    "        token_length = len(tokens)\n",
    "        #     print('Token length: ', str(token_length))\n",
    "        token_values = {}\n",
    "        for i in range(token_length):\n",
    "            # Get tfidf score for each token\n",
    "            token_name = tokens[i]\n",
    "            try:\n",
    "                if token_name in self.vectorizer.vocabulary_:\n",
    "                    token_index = self.vectorizer.vocabulary_[token_name]\n",
    "                    token_value = row[token_index]\n",
    "                else:\n",
    "                    token_value = 0\n",
    "            except:\n",
    "                print(\"Exception: \", str(row_id))\n",
    "            token_values[token_name] = token_value\n",
    "        # Sort the tokens by tfidf values\n",
    "        sorted_tokens = sorted(token_values.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        #     print(sorted_tokens)\n",
    "        # Get the most weighted tokens\n",
    "        top_tokens = []\n",
    "        padding_count = 0\n",
    "        #     print(\"Sorted tokens length: \", str(len(sorted_tokens)))\n",
    "        if len(sorted_tokens) < top_n:\n",
    "            padding_count = top_n - len(sorted_tokens)\n",
    "            for i in range(len(sorted_tokens)):\n",
    "                top_tokens.append(sorted_tokens[i][0])\n",
    "        else:\n",
    "            for i in range(top_n):\n",
    "                top_tokens.append(sorted_tokens[i][0])\n",
    "        for i in range(padding_count):\n",
    "            top_tokens.append('UNKNOWN')\n",
    "        return top_tokens\n",
    "\n",
    "    def convert_tokens_to_features(self, tokens):\n",
    "        default_embedding = np.zeros(self.embedding_dim, dtype=int).tolist()\n",
    "        features = []\n",
    "        for t in tokens:\n",
    "            if t in self.w2v_model:\n",
    "                features += self.w2v_model[t].tolist()\n",
    "            else:\n",
    "                features += default_embedding\n",
    "        return features\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        df['top_tokens'] = df.apply(lambda x: self.get_top_tokens_in_doc(df,\n",
    "                                                                       vectors=self.vectors,\n",
    "                                                                       row_id=x.name,\n",
    "                                                                       top_n=self.top_n_token),\n",
    "                                    axis=1\n",
    "                                  )\n",
    "        df['features'] = df['top_tokens'].apply(lambda x: self.convert_tokens_to_features(x))\n",
    "        df.to_csv('transformed_topn-{0}_embeddingdim-{1}.csv'.format(self.top_n_token, self.embedding_dim))\n",
    "        return df[['features', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvin/.conda/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:73: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/alvin/.conda/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:74: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "feature_pipeline = Pipeline([\n",
    "        ('Tokens_Picker_Pipeline', TokensPicker())\n",
    "])\n",
    "\n",
    "df_train_transformed = feature_pipeline.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.7738588452339172, 1.0097960233688354, -0.4...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.2603164315223694, 0.4835744798183441, -0.9...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.2622810900211334, 0.623058021068573, -0.42...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.4411155879497528, 0.5608793497085571, -0.2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.10545478761196136, 0.49129846692085266, -0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  class\n",
       "0  [-0.7738588452339172, 1.0097960233688354, -0.4...      2\n",
       "1  [-0.2603164315223694, 0.4835744798183441, -0.9...      2\n",
       "2  [-0.2622810900211334, 0.623058021068573, -0.42...      1\n",
       "3  [-0.4411155879497528, 0.5608793497085571, -0.2...      2\n",
       "4  [-0.10545478761196136, 0.49129846692085266, -0...      2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('Classifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=...     presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=4,\n",
       "       param_grid=[{'Classifier__n_estimators': [20, 50, 80, 100], 'Classifier__max_depth': [2, 4, 6, 8, 10], 'Classifier__min_samples_split': [200, 500, 800, 1000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='neg_log_loss',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [100, 200, 500, 1000]\n",
    "max_depth = [2, 4, 6, 8, 10]\n",
    "min_samples_split = [500, 1000, 2000]\n",
    "\n",
    "classifier_pipeline = Pipeline([\n",
    "#         ('Tokens_Picker_Pipeline', TokensPicker())\n",
    "#     ,\n",
    "        ('Classifier', GradientBoostingClassifier())\n",
    "    ])\n",
    "\n",
    "param_grid = [{\n",
    "#     'Tokens_Picker_Pipeline__embed_dim': embed_dims,\n",
    "#     'Tokens_Picker_Pipeline__top_n': top_ns,\n",
    "#     'Tokens_Picker_Pipeline__window': windows\n",
    "#     ,\n",
    "    'Classifier__n_estimators': n_estimators,\n",
    "    'Classifier__max_depth': max_depth,\n",
    "    'Classifier__min_samples_split': min_samples_split\n",
    "}]\n",
    "\n",
    "grid = GridSearchCV(classifier_pipeline, cv=5, n_jobs=4, param_grid=param_grid, scoring='neg_log_loss')\n",
    "grid.fit(df_train_transformed['features'].tolist(), df_train['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Classifier__max_depth': 6,\n",
       " 'Classifier__min_samples_split': 1000,\n",
       " 'Classifier__n_estimators': 100}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -1.20905, std: 0.02600, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 20},\n",
       " mean: -1.02622, std: 0.02806, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 50},\n",
       " mean: -0.96456, std: 0.02500, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 80},\n",
       " mean: -0.94400, std: 0.02847, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 100},\n",
       " mean: -1.23275, std: 0.02442, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 20},\n",
       " mean: -1.03369, std: 0.02537, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 50},\n",
       " mean: -0.96920, std: 0.02623, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 80},\n",
       " mean: -0.94670, std: 0.02651, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 100},\n",
       " mean: -1.24772, std: 0.02500, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 20},\n",
       " mean: -1.04495, std: 0.02020, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 50},\n",
       " mean: -0.97860, std: 0.02229, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 80},\n",
       " mean: -0.95211, std: 0.02328, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 100},\n",
       " mean: -1.25563, std: 0.02391, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 20},\n",
       " mean: -1.05529, std: 0.02134, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 50},\n",
       " mean: -0.98594, std: 0.02314, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 80},\n",
       " mean: -0.95736, std: 0.02390, params: {'Classifier__max_depth': 2, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 100},\n",
       " mean: -1.10213, std: 0.03067, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 20},\n",
       " mean: -0.94616, std: 0.03216, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 50},\n",
       " mean: -0.91452, std: 0.03689, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 80},\n",
       " mean: -0.90764, std: 0.03681, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 100},\n",
       " mean: -1.14257, std: 0.02688, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 20},\n",
       " mean: -0.95945, std: 0.02945, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 50},\n",
       " mean: -0.91537, std: 0.03232, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 80},\n",
       " mean: -0.90568, std: 0.03517, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 100},\n",
       " mean: -1.17429, std: 0.02657, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 20},\n",
       " mean: -0.98050, std: 0.02249, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 50},\n",
       " mean: -0.92591, std: 0.02049, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 80},\n",
       " mean: -0.91019, std: 0.02358, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 100},\n",
       " mean: -1.18539, std: 0.02968, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 20},\n",
       " mean: -0.99086, std: 0.02631, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 50},\n",
       " mean: -0.93234, std: 0.02577, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 80},\n",
       " mean: -0.91186, std: 0.02982, params: {'Classifier__max_depth': 4, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 100},\n",
       " mean: -1.06774, std: 0.03175, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 20},\n",
       " mean: -0.92842, std: 0.03422, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 50},\n",
       " mean: -0.91813, std: 0.03835, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 80},\n",
       " mean: -0.92446, std: 0.04341, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 100},\n",
       " mean: -1.11460, std: 0.02885, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 20},\n",
       " mean: -0.93503, std: 0.03126, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 50},\n",
       " mean: -0.90983, std: 0.03582, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 80},\n",
       " mean: -0.90742, std: 0.04015, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 100},\n",
       " mean: -1.14973, std: 0.02775, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 20},\n",
       " mean: -0.95850, std: 0.02796, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 50},\n",
       " mean: -0.91444, std: 0.03154, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 80},\n",
       " mean: -0.90283, std: 0.03304, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 100},\n",
       " mean: -1.16524, std: 0.02664, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 20},\n",
       " mean: -0.96820, std: 0.02738, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 50},\n",
       " mean: -0.91661, std: 0.03217, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 80},\n",
       " mean: -0.90281, std: 0.03610, params: {'Classifier__max_depth': 6, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 100},\n",
       " mean: -1.05914, std: 0.03162, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 20},\n",
       " mean: -0.93093, std: 0.03501, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 50},\n",
       " mean: -0.93859, std: 0.04598, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 80},\n",
       " mean: -0.96104, std: 0.04945, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 100},\n",
       " mean: -1.10592, std: 0.02767, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 20},\n",
       " mean: -0.93675, std: 0.03160, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 50},\n",
       " mean: -0.91557, std: 0.03801, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 80},\n",
       " mean: -0.92313, std: 0.04427, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 100},\n",
       " mean: -1.14117, std: 0.02389, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 20},\n",
       " mean: -0.95575, std: 0.02584, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 50},\n",
       " mean: -0.91774, std: 0.03313, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 80},\n",
       " mean: -0.91697, std: 0.03559, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 100},\n",
       " mean: -1.15486, std: 0.02660, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 20},\n",
       " mean: -0.96282, std: 0.03328, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 50},\n",
       " mean: -0.91497, std: 0.03449, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 80},\n",
       " mean: -0.90756, std: 0.03870, params: {'Classifier__max_depth': 8, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 100},\n",
       " mean: -1.05673, std: 0.03500, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 20},\n",
       " mean: -0.93338, std: 0.04544, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 50},\n",
       " mean: -0.96244, std: 0.04912, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 80},\n",
       " mean: -0.99570, std: 0.05546, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 200, 'Classifier__n_estimators': 100},\n",
       " mean: -1.09988, std: 0.02834, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 20},\n",
       " mean: -0.93527, std: 0.03298, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 50},\n",
       " mean: -0.92350, std: 0.04405, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 80},\n",
       " mean: -0.94359, std: 0.04099, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 500, 'Classifier__n_estimators': 100},\n",
       " mean: -1.13835, std: 0.02685, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 20},\n",
       " mean: -0.95439, std: 0.03251, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 50},\n",
       " mean: -0.92259, std: 0.03685, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 80},\n",
       " mean: -0.93117, std: 0.03601, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 800, 'Classifier__n_estimators': 100},\n",
       " mean: -1.15044, std: 0.02694, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 20},\n",
       " mean: -0.96202, std: 0.03064, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 50},\n",
       " mean: -0.91985, std: 0.03333, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 80},\n",
       " mean: -0.91743, std: 0.03903, params: {'Classifier__max_depth': 10, 'Classifier__min_samples_split': 1000, 'Classifier__n_estimators': 100}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvin/.conda/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:73: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/alvin/.conda/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:74: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "model_0 = feature_pipeline.fit(df_train, df_train['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvin/.conda/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:73: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/alvin/.conda/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:74: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, ..., 6, 5, 4])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.predict(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_transformed = df_train_transformed.drop(['class'], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x_transformed, df_train_transformed['class'], test_size=0.2, random_state=11, stratify=df_train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=10, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "gbm0.fit(x_train['features'].tolist(), y_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training set\n",
    "train_predictions = gbm0.predict(x_test['features'].tolist())\n",
    "train_predprob = gbm0.predict_proba(x_test['features'].tolist())[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 3, 3, 4, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.66337170e-04, 4.03305413e-04, 4.55247937e-03, 1.76416634e-01,\n",
       "        1.05184377e-02, 7.76706779e-01, 9.19678397e-03, 5.20889555e-03,\n",
       "        1.52967988e-02, 1.17838339e-03, 3.55165810e-04]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6460732984293194\n"
     ]
    }
   ],
   "source": [
    "acc_score = metrics.accuracy_score(y_test.tolist(), train_predictions)\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4869566001388137\n"
     ]
    }
   ],
   "source": [
    "auc_score = metrics.log_loss(y_train.tolist(), gbm0.predict_proba(x_train['features'].tolist()))\n",
    "print(auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
